# TODO
- Test [pre-trained roberta model](https://huggingface.co/MidhunKanadan/roberta-large-fallacy-classification) on logic and logicClimate dataset (baseline model, higher priority)
  - Imbalanced class distribution in Logic and LogicClimate datasets and different classes. Drop minority classes or oversample?
- Fine-tune Llama-3.3 on LFUD, train on Logic and LogicClimate datasets
- Generating more Logical Fallacy data based on LFUD Paper. This will give us enough data to fine-tune the larger models like DeepSeek R1 and GPT
